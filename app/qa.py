# qa.py
import os
import time
from typing import Tuple, Optional

from langchain_core.documents import Document

# embedding & vectorstore imports
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from config import USE_GROQ, EMBEDDING_MODEL
from llm import call_groq

# Optional LLM import (used only if OPENAI_API_KEY is present)
try:
    from langchain import OpenAI
    from langchain.prompts import PromptTemplate
    from langchain.chains import LLMChain
    OPENAI_AVAILABLE = True
except Exception:
    OPENAI_AVAILABLE = False

# default search top-k
DEFAULT_K = 3

def _safe_load_index(index_folder: str) -> Tuple[Optional[FAISS], Optional[str]]:
    """Try to load a local FAISS index. Return (store, error_message)."""
    if not index_folder:
        return None, "No index_folder provided."
    if not os.path.exists(index_folder):
        return None, f"Index path does not exist: {index_folder}"
    try:
        embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        store = FAISS.load_local(index_folder, embeddings, allow_dangerous_deserialization=True)
        return store, None
    except Exception as e:
        return None, f"Failed to load FAISS index: {e}"

def _docs_to_context(docs):
    """Create a short context string from documents (safe length)."""
    pieces = []
    for i, d in enumerate(docs):
        text = d.page_content or getattr(d, "content", "")
        # keep first 800 chars of each doc to keep prompt size reasonable
        pieces.append(f"-----\nSource {i+1}:\n{text[:800]}")
    return "\n\n".join(pieces)

def _call_openai(prompt: str, max_tokens: int = 256) -> str:
    """Call OpenAI LLM via langchain.OpenAI if available."""
    if not OPENAI_AVAILABLE:
        raise RuntimeError("OpenAI chain not available (missing langchain OpenAI).")
    # LangChain OpenAI will read OPENAI_API_KEY from env
    llm = OpenAI(temperature=0)
    prompt_template = PromptTemplate(input_variables=["context", "question"],
                                     template="You are a helpful assistant.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer concisely:")
    chain = LLMChain(llm=llm, prompt=prompt_template)
    # chain expects dict: {"context":..., "question":...}
    return chain.run({"context": prompt, "question": ""})  # we pass whole prompt as context; question blank

def generate_answer(query: str, index_folder: Optional[str] = None) -> str:
    """
    Generate answer for `query`.
    - If OPENAI_API_KEY is set and langchain.OpenAI is available, use it.
    - Otherwise, return extractive answer from top documents.
    Returns a string (answer or error message).
    """
    start = time.time()
    # 1) load index
    store, err = _safe_load_index(index_folder)
    if err:
        return f"[Error loading index] {err}"

    try:
        retriever = store.as_retriever(search_kwargs={"k": DEFAULT_K})
        docs = retriever.invoke(query)
    except Exception as e:
        return f"[Error during retrieval] {e}"

    if not docs:
        return "No relevant documents found in the selected index."

    # 2) If USE_GROQ is enabled, use Groq
    if USE_GROQ:
        try:
            context = _docs_to_context(docs)
            prompt_text = f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:"
            answer = call_groq(prompt_text)
            elapsed = time.time() - start
            
            # Extract unique sources
            sources = list(set([d.metadata.get("source", "Unknown") for d in docs]))
            source_list = "\n".join([f"- {s}" for s in sources])
            
            return f"{answer}\n\n**Sources:**\n{source_list}\n\n(generated by Groq in {elapsed:.2f}s)"
        except Exception as e:
            return f"[Groq call failed: {e}]"

    # If OpenAI key present in env and OpenAI is available, try generative answer
    if os.getenv("OPENAI_API_KEY") and OPENAI_AVAILABLE:
        try:
            # build short context from docs and ask model
            context = _docs_to_context(docs)
            # construct prompt: include question and context
            prompt_text = f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:"
            answer = _call_openai(prompt_text)
            elapsed = time.time() - start
            
            # Extract unique sources
            sources = list(set([d.metadata.get("source", "Unknown") for d in docs]))
            source_list = "\n".join([f"- {s}" for s in sources])
            
            return f"{answer}\n\n**Sources:**\n{source_list}\n\n(generated in {elapsed:.2f}s)"
        except Exception as e:
            # fallback to extractive
            fallback = _docs_to_context(docs[:1])
            return f"[OpenAI call failed: {e}]\n\nFallback (top doc excerpt):\n{fallback}"

    # Fallback path: no LLM available â€” return extractive summary (first doc excerpt + list)
    try:
        snippets = []
        for i, d in enumerate(docs[:3]):
            text = d.page_content or getattr(d, "content", "")
            # take first 1000 characters, preserve sentence break if possible
            snippet = text[:1000].strip()
            snippets.append(f"Source {i+1} excerpt:\n{snippet}")
        elapsed = time.time() - start
        return "\n\n".join(snippets) + f"\n\n[Returned {len(docs)} docs] (extractive fallback in {elapsed:.2f}s)"
    except Exception as e:
        return f"[Error preparing extractive answer] {e}"
